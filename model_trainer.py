import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import logging
from typing import Dict, Tuple, Any, List
import optuna

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
LOOKBACK_WINDOW = 30
FORECAST_DAYS = 30


class ModelTrainer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.models = {}
        self.metrics = {}
        self.test_data = None
        self.train_test_split_date = None

    def prepare_ml_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """–£–ù–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–ê–Ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        self.logger.info("–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–∞—Ç—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        split_idx = int(len(data) * 0.8)
        self.train_test_split_date = data.index[split_idx]

        self.logger.info(f"–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {self.train_test_split_date}")

        features = data.drop(['price'], axis=1)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        target = data['price']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—É –∫–∞–∫ —Ç–∞—Ä–≥–µ—Ç

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∞—Ç–µ
        train_mask = data.index < self.train_test_split_date
        test_mask = data.index >= self.train_test_split_date

        X_train, X_test = features[train_mask], features[test_mask]
        y_train, y_test = target[train_mask], target[test_mask]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ARIMA
        self.test_data = data[test_mask]

        self.logger.info(f"–î–∞–Ω–Ω—ã–µ: {len(X_train)} train, {len(X_test)} test")

        return X_train.values, X_test.values, y_train.values, y_test.values

    def _create_features_without_leakage(self, data: pd.Series) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ë–ï–ó —É—Ç–µ—á–∫–∏ –≤ –±—É–¥—É—â–µ–µ"""
        df = pd.DataFrame({'price': data})

        # –¢–û–õ–¨–ö–û –ª–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
        for i in range(1, 8):
            df[f'lag_{i}'] = df['price'].shift(i)

        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–∏ –¢–û–õ–¨–ö–û –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df['ma_7'] = df['price'].shift(1).rolling(window=7).mean()
        df['ma_14'] = df['price'].shift(1).rolling(window=14).mean()
        df['ma_30'] = df['price'].shift(1).rolling(window=30).mean()

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month

        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - —Å–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        df['target'] = df['price'].shift(-1)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df = df.dropna()

        self.logger.info(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫, {len(df)} —Å—Ç—Ä–æ–∫")
        return df

    def train_random_forest(self, X_train: np.ndarray, X_test: np.ndarray,
                            y_train: np.ndarray, y_test: np.ndarray) -> Tuple[Any, Dict]:
        """–û–±—É—á–µ–Ω–∏–µ Random Forest —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        try:
            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ Random Forest...")

            model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)
            metrics = self._calculate_metrics(y_test, y_pred)

            self.models['random_forest'] = model
            self.metrics['random_forest'] = metrics

            self.logger.info(f"‚úÖ Random Forest: RMSE={metrics['rmse']:.2f}, MAPE={metrics['mape']:.1f}%")
            return model, metrics
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Random Forest: {str(e)}")
            return None, self._get_default_metrics()

    def train_ridge_regression(self, X_train: np.ndarray, X_test: np.ndarray,
                               y_train: np.ndarray, y_test: np.ndarray) -> Tuple[Any, Dict]:
        """–û–±—É—á–µ–Ω–∏–µ Ridge Regression —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        try:
            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ Ridge Regression...")

            model = Ridge(alpha=1.0, random_state=42)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)
            metrics = self._calculate_metrics(y_test, y_pred)

            self.models['ridge'] = model
            self.metrics['ridge'] = metrics

            self.logger.info(f"‚úÖ Ridge: RMSE={metrics['rmse']:.2f}, MAPE={metrics['mape']:.1f}%")
            return model, metrics
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ridge: {str(e)}")
            return None, self._get_default_metrics()

    def train_arima(self, data: pd.Series) -> Tuple[Any, Dict]:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ ARIMA —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        try:
            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ ARIMA —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")

            if self.train_test_split_date is None:
                raise ValueError("–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –¥–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")

            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–æ–π –∂–µ –¥–∞—Ç–µ —á—Ç–æ –∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            train_data = data[data.index < self.train_test_split_date]
            test_data = data[data.index >= self.train_test_split_date]

            self.logger.info(f"üìä ARIMA: {len(train_data)} train, {len(test_data)} test")

            if len(train_data) < 30 or len(test_data) < 10:
                raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ARIMA")

            # –û–±—É—á–∞–µ–º ARIMA –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
            model = ARIMA(train_data, order=(1, 1, 1))
            fitted_model = model.fit()

            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –Ω–∞ –¥–ª–∏–Ω—É test –ø–µ—Ä–∏–æ–¥–∞
            forecast_steps = len(test_data)
            forecast = fitted_model.forecast(steps=forecast_steps)

            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
            min_len = min(len(test_data), len(forecast))
            test_data_aligned = test_data.iloc[:min_len]
            forecast_aligned = forecast[:min_len]

            metrics = self._calculate_metrics(test_data_aligned.values, forecast_aligned.values)

            self.models['arima'] = fitted_model
            self.metrics['arima'] = metrics

            self.logger.info(f"‚úÖ ARIMA: RMSE={metrics['rmse']:.2f}, MAPE={metrics['mape']:.1f}%")
            return fitted_model, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ ARIMA: {str(e)}")
            return None, self._get_default_metrics()

    def train_lstm(self, data: pd.Series) -> Tuple[Any, Dict]:
        """–û–±—É—á–µ–Ω–∏–µ LSTM —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        try:
            from sklearn.preprocessing import MinMaxScaler

            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ LSTM —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")

            if self.train_test_split_date is None:
                raise ValueError("–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –¥–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")

            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            scaler = MinMaxScaler(feature_range=(0, 1))
            scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))

            def create_sequences(data, seq_length):
                X, y = [], []
                for i in range(seq_length, len(data)):
                    X.append(data[i - seq_length:i, 0])
                    y.append(data[i, 0])
                return np.array(X), np.array(y)

            seq_length = 30
            X, y = create_sequences(scaled_data, seq_length)

            # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
            dates = data.index[seq_length:]
            train_mask = dates < self.train_test_split_date
            test_mask = dates >= self.train_test_split_date

            X_train, X_test = X[train_mask], X[test_mask]
            y_train, y_test = y[train_mask], y[test_mask]

            self.logger.info(f"üìä LSTM: {len(X_train)} train, {len(X_test)} test")

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

            # –°–æ–∑–¥–∞–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏
            model = Sequential([
                LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),
                Dropout(0.2),
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25),
                Dense(1)
            ])

            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(
                X_train, y_train,
                batch_size=32,
                epochs=50,
                validation_data=(X_test, y_test),
                verbose=0
            )

            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            y_pred_scaled = model.predict(X_test, verbose=0)
            y_pred = scaler.inverse_transform(y_pred_scaled)
            y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))

            metrics = self._calculate_metrics(y_test_orig.flatten(), y_pred.flatten())

            self.models['lstm'] = {
                'model': model,
                'scaler': scaler,
                'seq_length': seq_length
            }
            self.metrics['lstm'] = metrics

            self.logger.info(f"‚úÖ LSTM: RMSE={metrics['rmse']:.2f}, MAPE={metrics['mape']:.1f}%")
            return model, metrics

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ LSTM: {str(e)}")
            return None, self._get_default_metrics()

    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """–ù–∞–¥–µ–∂–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            if len(y_true) == 0 or len(y_pred) == 0:
                return self._get_default_metrics()

            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
            min_len = min(len(y_true), len(y_pred))
            y_true = y_true[:min_len]
            y_pred = y_pred[:min_len]

            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = np.mean(np.abs(y_true - y_pred))

            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π MAPE
            mask = (y_true != 0) & (np.abs(y_true) > 0.001)
            if np.sum(mask) > 0:
                mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
            else:
                mape = 0.0

            return {'rmse': rmse, 'mape': mape, 'mae': mae}

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫: {str(e)}")
            return self._get_default_metrics()

    def _get_default_metrics(self) -> Dict:
        """–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {'rmse': float('inf'), 'mape': 100.0, 'mae': float('inf')}

    def select_best_model(self) -> Tuple[str, Dict]:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ RMSE"""
        if not self.metrics:
            raise ValueError("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞")

        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –í–°–ï–• –º–æ–¥–µ–ª–µ–π
        self.logger.info("üìä –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ï–ô:")
        for model_name, metrics in self.metrics.items():
            if metrics and metrics['rmse'] < float('inf'):
                self.logger.info(
                    f"   {model_name.upper():<15} RMSE: {metrics['rmse']:.2f} MAPE: {metrics['mape']:.1f}% MAE: {metrics['mae']:.2f}")

        best_model = None
        best_rmse = float('inf')

        for model_name, metrics in self.metrics.items():
            if metrics and metrics['rmse'] < best_rmse:
                best_rmse = metrics['rmse']
                best_model = model_name

        if best_model is None:
            # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å –æ—à–∏–±–∫–∞–º–∏, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
            for model_name in self.metrics.keys():
                best_model = model_name
                break

        if best_model:
            self.logger.info(f"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model.upper()} (RMSE={best_rmse:.2f})")
            return best_model, self.metrics[best_model]
        else:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å")

    def generate_forecast(self, best_model_name: str, data: pd.Series, days: int = 30) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        try:
            self.logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –º–æ–¥–µ–ª—å—é {best_model_name} –Ω–∞ {days} –¥–Ω–µ–π")

            if data.empty or len(data) < 50:
                raise ValueError(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(data)} –∑–∞–ø–∏—Å–µ–π")

            if best_model_name == 'random_forest':
                return self._forecast_random_forest(data, days)
            elif best_model_name == 'ridge':
                return self._forecast_ridge(data, days)
            elif best_model_name == 'arima':
                return self._forecast_arima(data, days)
            elif best_model_name == 'lstm':
                return self._forecast_lstm(data, days)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")
            return self._fallback_forecast(data, days)

    def _forecast_random_forest(self, data: pd.Series, days: int) -> List[float]:
        """–ü—Ä–æ–≥–Ω–æ–∑ Random Forest"""
        if 'random_forest' not in self.models:
            raise ValueError("Random Forest –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        model = self.models['random_forest']

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        features_df = self._create_features_for_forecast(data)

        forecast = []
        current_features = features_df.iloc[-1:].copy()

        for i in range(days):
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            next_price = model.predict(current_features.values)[0]
            forecast.append(float(next_price))

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
            current_features = self._update_features_for_next_prediction(
                current_features, next_price, data.index[-1] + pd.Timedelta(days=i + 1)
            )

        return forecast

    def _forecast_ridge(self, data: pd.Series, days: int) -> List[float]:
        """–ü—Ä–æ–≥–Ω–æ–∑ Ridge Regression"""
        if 'ridge' not in self.models:
            raise ValueError("Ridge –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        model = self.models['ridge']

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        features_df = self._create_features_for_forecast(data)

        forecast = []
        current_features = features_df.iloc[-1:].copy()

        for i in range(days):
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            next_price = model.predict(current_features.values)[0]
            forecast.append(float(next_price))

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
            current_features = self._update_features_for_next_prediction(
                current_features, next_price, data.index[-1] + pd.Timedelta(days=i + 1)
            )

        return forecast

    def _forecast_arima(self, data: pd.Series, days: int) -> List[float]:
        """–ü—Ä–æ–≥–Ω–æ–∑ ARIMA"""
        if 'arima' not in self.models:
            raise ValueError("ARIMA –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        model = self.models['arima']
        forecast = model.forecast(steps=days)
        return forecast.tolist()

    def _forecast_lstm(self, data: pd.Series, days: int) -> List[float]:
        """–ü—Ä–æ–≥–Ω–æ–∑ LSTM"""
        if 'lstm' not in self.models:
            raise ValueError("LSTM –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        model_info = self.models['lstm']
        model = model_info['model']
        scaler = model_info['scaler']
        seq_length = model_info['seq_length']

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        scaled_data = scaler.transform(data.values.reshape(-1, 1))

        forecast = []
        current_sequence = scaled_data[-seq_length:].reshape(1, seq_length, 1)

        for i in range(days):
            next_pred = model.predict(current_sequence, verbose=0)
            next_price_scaled = next_pred[0][0]
            next_price = scaler.inverse_transform([[next_price_scaled]])[0][0]
            forecast.append(float(next_price))

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            next_scaled = scaler.transform([[next_price]])
            current_sequence = np.append(current_sequence[:, 1:, :],
                                         next_scaled.reshape(1, 1, 1), axis=1)

        return forecast

    def _create_features_for_forecast(self, data: pd.Series) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        df = pd.DataFrame({'price': data})

        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for i in range(1, 8):
            df[f'lag_{i}'] = df['price'].shift(i)

        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        df['ma_7'] = df['price'].rolling(window=7).mean()
        df['ma_14'] = df['price'].rolling(window=14).mean()
        df['ma_30'] = df['price'].rolling(window=30).mean()

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['day_of_week'] = df.index.dayofweek
        df['month'] = df.index.month

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.fillna(method='bfill').fillna(method='ffill')

        return df.drop(['price'], axis=1)

    def _update_features_for_next_prediction(self, current_features: pd.DataFrame,
                                             next_price: float, next_date: pd.Timestamp) -> pd.DataFrame:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞"""
        new_features = current_features.copy()

        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for i in range(6, 0, -1):
            new_features[f'lag_{i + 1}'] = new_features[f'lag_{i}']
        new_features['lag_1'] = next_price

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
        current_ma7 = new_features['ma_7'].iloc[0]
        new_ma7 = (current_ma7 * 6 + next_price) / 7
        new_features['ma_7'] = new_ma7

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        new_features['day_of_week'] = next_date.dayofweek
        new_features['month'] = next_date.month

        return new_features

    def _calculate_trend(self, data: pd.Series) -> float:
        """–†–∞—Å—á–µ—Ç —Ç—Ä–µ–Ω–¥–∞"""
        if len(data) < 2:
            return 0.0

        changes = np.diff(data.values)
        return np.mean(changes) if len(changes) > 0 else 0.0

    def _fallback_forecast(self, data: pd.Series, days: int) -> List[float]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑"""
        try:
            self.logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ —Ç—Ä–µ–Ω–¥—É")

            last_price = data.iloc[-1]
            trend = self._calculate_trend(data.tail(30))

            forecast = []
            current_price = last_price

            for i in range(days):
                change = trend * 0.8
                next_price = max(0.1, current_price + change)
                forecast.append(float(next_price))
                current_price = next_price

            return forecast

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")
            return [float(data.iloc[-1])] * days